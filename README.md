# AI Award Interpreter

Transform Fair Work Awards into System Configurations automatically using AI.

## Overview

This tool analyzes new or updated Fair Work awards and automatically generates compatible configurations for your existing wage calculation system.

### One-Time Setup (Done Once)

Before processing awards, the system requires these baseline templates to be stored:

1. **ğŸ“„ Baseline Config Template** (`data/baseline_config.json`)
   - Your existing JSON configuration (e.g., Workpac_Input.json)
   - Defines how rules are currently configured
   - Contains: Shifts, PayComponents, AwardVariation, Allowances, etc.

2. **ğŸ Python Script Template** (Reference only)
   - Your existing Python wage calculation script (e.g., WorkpacNonCoal+Clerks_PYscript.py)
   - Used for understanding calculation logic
   - Not directly modified by the tool

3. **ğŸ“ Internal Rule Model** (`models/__init__.py` - AwardSpec)
   - Standard format for representing any award
   - Contains: OrdinaryHours, OvertimeRules, PenaltyRates, Allowances, BreakRules, PublicHolidayRules
   - AI extracts new awards into this consistent format

Once these baselines are configured, you can process unlimited awards against them.

---

## Features

- ğŸ” **Automatic Award Parsing**: Fetches and parses awards from fwc.gov.au
- ğŸ¤– **AI-Powered Rule Extraction**: Uses GPT-4 to extract wage calculation rules
- ğŸ“Š **Gap Analysis**: Identifies config-only vs code-required changes
- ğŸ¯ **Smart Ambiguity Detection**: Flags items needing human clarification
- ğŸ“¦ **Automatic Config Generation**: Generates updated JSON configurations
- ğŸ”§ **Patch Plan Generation**: Suggests Python code modifications
- ğŸ’° **Cost Tracking**: Monitors OpenAI API costs per session

## Installation

1. **Clone or download this project**

2. **Install dependencies**:
   ```bash
   cd ai_award_interpreter
   pip install -r requirements.txt
   ```

3. **Create `.env` file** in the `ai_award_interpreter` directory:
   ```env
   api_key=your_openai_api_key_here
   ```

4. **Copy baseline configuration**:
   Copy your existing `Workpac_Input.json` to `data/baseline_config.json`:
   ```bash
   cp ../Workpac_Input.json data/baseline_config.json
   ```

## Usage

1. **Start the app**:
   ```bash
   streamlit run app.py
   ```

2. **Open in browser**: http://localhost:8501

3. **Process an award**:
   - Enter award URL (e.g., `https://awards.fairwork.gov.au/MA000028.html`)
   - Click "Start Analysis"
   - Wait for processing (2-5 minutes)
   - Resolve any ambiguities
   - Review outputs
   - Download updated JSON config

## Example Awards

- **Horticulture Award**: https://awards.fairwork.gov.au/MA000028.html
- **Clerks Award**: https://awards.fairwork.gov.au/MA000002.html
- **Mining Award**: https://awards.fairwork.gov.au/MA000027.html

## Cost Estimates

- Typical award: $0.40 - $0.65 per run
- Monthly budget: $100 (150-250 awards)
- Cost tracking displayed in UI

## Output Files

Each session generates:

1. **updated_config.json**: New JSON configuration
2. **patch_plan.md**: Python modification suggestions
3. **gap_report.json**: Detailed gap analysis
4. **award_spec.json**: Extracted award specification

All files stored in: `sessions/sess-YYYYMMDD-HHMMSS-XXXXXX/`

## Project Structure

```
ai_award_interpreter/
â”œâ”€â”€ app.py                      # Streamlit UI
â”œâ”€â”€ config.py                   # Configuration
â”œâ”€â”€ requirements.txt            # Dependencies
â”‚
â”œâ”€â”€ core/
â”‚   â””â”€â”€ orchestrator.py         # Main workflow
â”‚
â”œâ”€â”€ ingestion/
â”‚   â”œâ”€â”€ award_fetcher.py        # Fetch from Fair Work
â”‚   â”œâ”€â”€ html_parser.py          # Parse HTML
â”‚   â”œâ”€â”€ clause_chunker.py       # Chunk clauses
â”‚   â””â”€â”€ vector_store.py         # ChromaDB interface
â”‚
â”œâ”€â”€ extraction/
â”‚   â””â”€â”€ rule_extractor.py       # LLM rule extraction
â”‚
â”œâ”€â”€ analysis/
â”‚   â””â”€â”€ gap_analyzer.py         # Gap analysis
â”‚
â”œâ”€â”€ generation/
â”‚   â”œâ”€â”€ json_generator.py       # JSON config generation
â”‚   â””â”€â”€ patch_generator.py      # Patch plan generation
â”‚
â”œâ”€â”€ models/
â”‚   â””â”€â”€ __init__.py             # Data models
â”‚
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ openai_client.py        # OpenAI wrapper
â”‚   â””â”€â”€ prompt_templates.py     # LLM prompts
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ baseline_config.json    # Your current config
â”‚
â””â”€â”€ sessions/                   # Session outputs
```

## Troubleshooting

### Error: "api_key not found in environment variables"
- Ensure `.env` file exists in `ai_award_interpreter` directory
- Check that `.env` contains: `api_key=sk-...`

### Error: "Failed to fetch award"
- Check internet connection
- Verify award URL is correct and accessible
- Try a different award URL

### High costs
- Each award costs $0.40-$0.65
- Cost is tracked and displayed in UI
- Budget limit set to $100/month in config

### Extraction errors
- Some awards may have non-standard HTML structure
- Check `sessions/*/clauses.json` to verify parsing
- Report issues with specific award URLs

## Future Enhancements

- [ ] Version tracking (compare award versions)
- [ ] Edit extracted rules before applying
- [ ] Automated testing generation
- [ ] Batch processing
- [ ] PDF award support
- [ ] Code auto-patching (risky!)

## Support

For issues or questions, refer to the design document: `AI_AWARD_INTERPRETER_DESIGN.md`

## License

Internal development tool - not for redistribution.
