# Config Generator Options

The AI Award Interpreter now supports two different approaches for generating JSON configurations from award specifications.

## 1. Rule-Based Generator (Default)
**File:** `generation/json_generator.py`  
**Class:** `ConfigGenerator`

### How It Works
- Uses deterministic Python logic to transform award specifications into JSON configurations
- Directly maps AwardSpec fields to JSON structure using predefined templates
- Fast and predictable

### Advantages
- âš¡ **Fast**: No LLM API calls during config generation
- ğŸ’° **Cost-effective**: Zero additional OpenAI costs
- ğŸ¯ **Deterministic**: Same input always produces same output
- ğŸ› **Easy to debug**: Pure Python logic, can step through with debugger
- âœ… **Reliable**: No dependency on LLM availability or quality

### Disadvantages
- ğŸ”’ **Less flexible**: Requires code changes to handle new edge cases
- ğŸ“ **Template-based**: Limited to predefined configuration patterns
- ğŸ¤” **Manual mapping**: Someone needs to understand both award rules and JSON structure

### When to Use
- Production environments where consistency is critical
- Cost-sensitive applications
- Awards that fit well into standard patterns
- When you need fast, reliable results

---

## 2. LLM-Based Generator (Optional)
**File:** `generation/json_generator_llm.py`  
**Class:** `ConfigGeneratorLLM`

### How It Works
- Uses OpenAI's structured outputs with Pydantic models
- Sends award specification and baseline examples to GPT-4
- LLM generates complete JSON configuration following the schema
- Validates output against Pydantic models

### Advantages
- ğŸ§  **Intelligent**: Can handle complex, unusual award structures
- ğŸ”„ **Adaptive**: May find better mappings than hardcoded rules
- ğŸ“ **Context-aware**: Understands award language and intent
- ğŸ†• **Flexible**: Can handle new award patterns without code changes
- ğŸ” **Self-documenting**: Prompt explains the entire structure

### Disadvantages
- ğŸŒ **Slower**: Additional LLM API call (~5-10 seconds)
- ğŸ’¸ **More expensive**: Extra $0.02-0.05 per generation
- ğŸ² **Non-deterministic**: May vary slightly between runs
- ğŸ”Œ **Requires API**: Depends on OpenAI availability
- ğŸ› **Harder to debug**: LLM reasoning is not directly observable

### When to Use
- Complex awards that don't fit standard patterns
- Prototyping or experimentation
- When accuracy is more important than speed
- Awards with unusual rule combinations

---

## How to Switch Between Generators

### In Code
```python
from core.orchestrator import Orchestrator

# Use rule-based generator (default)
orchestrator = Orchestrator(use_llm_generator=False)

# Use LLM-based generator
orchestrator = Orchestrator(use_llm_generator=True)
```

### In Streamlit UI
1. Enter award URL
2. Check the "ğŸ¤– Use LLM Generator" checkbox to use AI-based generation
3. Leave unchecked for rule-based generation (default)
4. Click "ğŸš€ Start Analysis"

---

## Cost Comparison

### Rule-Based Generator
```
Embedding:     ~$0.01
Extraction:    ~$0.05  (single structured call)
Gap Analysis:  ~$0.03
Generation:    $0.00   (no LLM call)
Patch Plan:    ~$0.02
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Total:         ~$0.11 per award
```

### LLM-Based Generator
```
Embedding:     ~$0.01
Extraction:    ~$0.05  (single structured call)
Gap Analysis:  ~$0.03
Generation:    ~$0.03  (LLM-based)
Patch Plan:    ~$0.02
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Total:         ~$0.14 per award
```

**Difference:** LLM generator adds ~$0.03 (27% increase)

---

## Performance Comparison

### Rule-Based Generator
- Config generation: **Instant** (<100ms)
- Total pipeline: ~10-15 seconds

### LLM-Based Generator
- Config generation: **5-10 seconds** (LLM call)
- Total pipeline: ~15-25 seconds

---

## Recommendations

### Use Rule-Based Generator When:
- âœ… Processing standard awards (most Fair Work awards)
- âœ… Running in production with high volume
- âœ… Cost optimization is important
- âœ… Consistency/auditability is required
- âœ… Fast response time is needed

### Use LLM-Based Generator When:
- âœ… Award has unique/complex structure
- âœ… Rule-based output needs refinement
- âœ… Exploring new award patterns
- âœ… Prototyping or testing
- âœ… Accuracy matters more than speed/cost

### Hybrid Approach
You can also use both:
1. Start with rule-based generator for speed
2. If gaps are detected, re-run with LLM generator
3. Compare outputs and choose the better one

---

## Technical Details

### Pydantic Models (LLM Generator)
The LLM generator uses strict Pydantic schemas:
- `AwardVariationConfig`: Main award settings
- `AwardVariationRate`: Individual rate configurations
- `RateProperty`: Rate property flags
- `ShiftRule`: Shift-specific rules
- `CompleteConfig`: Root configuration object

This ensures:
- Type safety
- Automatic validation
- Structured LLM outputs
- Schema enforcement

### Prompt Engineering
The LLM prompt includes:
- Complete award specification
- Baseline configuration examples
- Detailed generation instructions
- Naming conventions and constraints
- Field-by-field guidance

---

## Future Improvements

### Potential Enhancements:
1. **Hybrid Generator**: Combine rule-based + LLM for best of both
2. **Confidence Scoring**: LLM rates its own output confidence
3. **Validation Layer**: Compare rule-based vs LLM outputs, flag discrepancies
4. **Caching**: Cache LLM generations for identical award specs
5. **Fine-tuned Model**: Train custom model on awardâ†’config mappings

---

## Conclusion

Both generators produce valid JSON configurations. Choose based on your priorities:
- **Speed + Cost**: Rule-based (default)
- **Flexibility + Intelligence**: LLM-based (optional)

The system is designed to make switching seamless, so you can experiment and choose what works best for your use case.
